<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> blog - page 3 | Sihyeong Yeom </title> <meta name="author" content="Sihyeong Yeom"> <meta name="description" content="M.S. Candidate in AI @ Konkuk Univ. | Focus on LLM, RAG, and Efficient Reasoning "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yeomsee.github.io/blog/page/3/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sihyeong</span> Yeom </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <div class="header-bar"> <h1>Sihyeong's Tech Blog</h1> <h2>논문 서베이 및 관심 연구 흐름 정리</h2> </div> <div class="tag-category-list"> <ul class="p-0 m-0"> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/blockquotes">blockquotes</a> </li> <p>•</p> <li> <i class="fa-solid fa-tag fa-sm"></i> <a href="/blog/category/external-services">external-services</a> </li> </ul> </div> <br> <div class="container featured-posts"> <div class="row row-cols-2"> <div class="col mb-4"> <a href="/blog/2021/distill/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a distill-style blog post</h3> <p class="card-text">an example of a distill-style blog post and main elements</p> <p class="post-meta"> 25 min read   ·   <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </div> </div> </div> </div> </a> </div> <div class="col mb-4"> <a href="/blog/2015/code/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a post with code</h3> <p class="card-text">an example of a blog post with some code</p> <p class="post-meta"> 4 min read   ·   <a href="/blog/2015"> <i class="fa-solid fa-calendar fa-sm"></i> 2015 </a> </p> </div> </div> </div> </div> </a> </div> </div> </div> <hr> <ul class="post-list"> <li> <h3> <a class="post-title" href="https://velog.io/@yeomsee97/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-SELF-RAG-Learning-to-Retrieve-Generate-and-Critique-through-Self-Reflection" target="_blank" rel="external nofollow noopener">[논문 리뷰] SELF-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <h2 id="problem-definition">Problem Definition</h2> <ul> <li>RAG는 LLM의 사실성 문제(e.g. hallucination)과 같은 문제를 해결할 수 있는 방법론<ul> <li>하지만, 기존 RAG의 <strong>무차별적 검색</strong>은 오히려 LLM의 다재다능함을 악화시킴<ul> <li>검색이 필요 없는 경우에도 고정적으로 k개의 문서를 검색해오기 때문에 query와 관련이 없는 문서가 검색될 수 있음</li> </ul> </li> <li>또한, RAG의 output이 <strong>일관성이 보장되지 않는다</strong>는 문제가 있음<ul> <li>즉, generation quality를 일관적으로 어떻다라고 말할 수가 없음</li> </ul> </li> </ul> </li> </ul> <hr> <h2 id="self-rag">SELF-RAG</h2> <ul> <li> <strong>on-demand retrieval + self-relection</strong> : <strong>반영 토큰(reflection token)</strong>으로 검색 여부를 결정하고 생성된 응답을 평가할 수 있음 → 반영 토큰에는 <strong>검색 토큰(retrieve token)</strong>, <strong>비평 토큰(critique token)</strong>이 있음 → 비평 토큰에는 Table1에서와 같이 <strong>IsREL, IsSUP, IsUSE 토큰</strong>이 있음</li> </ul> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/64a42a05-c874-407d-a9b2-9abb55b12790/image.PNG" alt=""></p> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/fcd3bbe3-e91f-4937-b7c6-73369c836402/image.PNG" alt=""></p> <hr> <h2 id="training-overview">Training Overview</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/f9a11bb0-015f-43c3-9037-de63cc98c92e/image.PNG" alt=""></p> <h3 id="training-critic-model">Training Critic Model</h3> <p>반영 토큰을 생성하도록 훈련 반영 토큰을 삽입한 훈련 데이터셋을 <strong>오프라인</strong>으로 업데이트</p> <h4 id="supervised-data-collection">(Supervised) Data Collection</h4> <ol> <li>무작위로 원본 데이터를 추출</li> <li>GPT-4에 반영 토큰별로 각기 다른 instruction을 줘서 결과 생성</li> <li>각 반영 토큰별 4k~20k의 훈련 데이터를 만들고 합침</li> <li>합쳐진 훈련 데이터를 바탕으로 비평 모델 학습</li> </ol> <h4 id="critic-learning">Critic Learning</h4> <ul> <li>비평 모델은 사전 학습된 LM이기만 하면 됨. 저자는 generator LM과 동일한 Llama2-7B 사용<ul> <li>위 과정에서 만든 학습 데이터로 <strong>conditional language modeling objective($p(r|I,x,y)$)</strong>과 같이 반영 토큰의 likelihood가 최대화되도록 학습<ul> <li>즉, 조건을 기반으로 반영 토큰 예측</li> <li>입력(조건)이 별도로 주어짐</li> </ul> </li> </ul> </li> </ul> <h3 id="training-generator-model">Training Generator Model</h3> <p>임의의 언어 모델이 반영 토큰과 함께 텍스트를 생성할 수 있도록 함 확장된 데이터셋(원래 데이터셋 + 반영 토큰)을 사용하여 다음 토큰 예측</p> <h4 id="data-collection">Data Collection</h4> <ul> <li>비평 모델 학습 시 만들었던 데이터를 바탕으로 학습</li> </ul> <h4 id="generator-learning">Generator Learning</h4> <ul> <li>critic model을 학습하는 방법과 똑같다. 단지, critic model과 달리, generator 모델이기 때문에 task에 대한 출력 및 반영 토큰을 예측하도록 학습<ul> <li> <strong>Next Token Prediction Objective</strong>(=GPT에서 흔히 사용되는 <strong>Autoregressive Modeling, $p(y,r|x)$</strong>)과 같이 마찬가지로 likelihood가 최대화되도록 학습</li> <li>이전 토큰을 기반으로 다음 토큰 예측</li> <li>입력이 곧 조건, 출력은 다음 토큰</li> </ul> </li> <li>loss 계산의 경우 검색된 텍스트는 마스킹<ul> <li>generator가 입력과 reflection token을 기반으로 출력하게끔 유도</li> </ul> </li> </ul> <hr> <h2 id="inference-overview">Inference Overview</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/9eb41051-1aa6-4730-96c7-536266f5c65f/image.PNG" alt=""></p> <ul> <li>입력에 대한 출력 먼저 생성</li> <li>이후 검색 토큰을 생성함으로써 검색 필요 유무를 결정<ul> <li>검색이 필요하지 않은 경우<ul> <li>언어 모델은 다음 출력 세그먼트를 예측(=일반적인 언어 모델의 구조)</li> </ul> </li> <li>검색이 필요한 경우<ul> <li>관련 문서 검색</li> <li>입력과 문서와의 관련성 평가(IsREL 토큰)</li> <li>문서와 응답의 지지도 평가(IsSUP 토큰) + 전반적인 유용성 평가(IsUSE 토큰) <em>(여러 개의 세그먼트를 동시에 생성하기 위해, 여러 개의 문서를 병렬적으로 처리)</em> </li> </ul> </li> </ul> </li> </ul> <hr> <h2 id="experiments">Experiments</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/8126ca6e-caea-49d2-8e19-8f096c42c950/image.PNG" alt=""></p> <ul> <li> <p>태스크/데이터셋</p> <ul> <li>closed-set tasks .. <em>분류 태스크?</em><ul> <li>사실 검증: PubHealth</li> <li>다지선다 추론: ARC-Challenge</li> </ul> </li> <li>단문 생성: Pop-QA, TriviaQA-unfiltered</li> <li>장문 생성<ul> <li>전기(biography) 생성: <a href="https://arxiv.org/pdf/2305.14251" rel="external nofollow noopener" target="_blank">FActScore</a> 참고</li> <li>QA: ALCE-ASQA</li> </ul> </li> </ul> </li> <li> <p>평가지표</p> <ul> <li>closed-set task, 단문 생성<ul> <li>accuracy(acc)</li> </ul> </li> <li>장문 생성<ul> <li>FactScore(FS): 전기 생성 측정</li> <li>str-em(em), rouge(rg): MAUVE를 기반으로 correctness 측정</li> <li>MAUVE(mau): MAUVE를 기반으로 fluency 측정</li> <li>citation precision(pre), recall(rec): ASQA에서 사용</li> </ul> </li> </ul> </li> <li> <p>결과</p> <ul> <li>w/ retrieval baseline과의 비교<ul> <li>SELF-RAG(7B, 13B)가 모든 부분에서 최고의 점수</li> <li>심지어, PubHealth, PopQA, 전기 생성, ASQA에서는 ChatGPT 능가</li> <li>전기 생성에서는 CoVE 능가</li> </ul> </li> <li>w/o retrieval baseline과의 비교<ul> <li>non-proprietary 언어 모델들보다 모든 부분에서 능가</li> <li>SELF-RAG는 ChatGPT를 제외한 모든 모델보다 높은 pre, rec 기록</li> <li>심지어 rec에서는 ChatGPT 능가</li> </ul> </li> </ul> </li> </ul> <p class="post-meta"> 1 min read   ·   December 30, 2024   ·   velog </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/velog"> <i class="fa-solid fa-hashtag fa-sm"></i> velog</a>   <a href="/blog/tag/technical"> <i class="fa-solid fa-hashtag fa-sm"></i> technical</a>   ·   <a href="/blog/category/external-posts"> <i class="fa-solid fa-tag fa-sm"></i> external-posts</a> </p> </li> <li> <div class="row"> <div class="col-sm-9"> <h3> <a class="post-title" href="/blog/2024/photo-gallery/">a post with image galleries</a> </h3> <p>this is what included image galleries could look like</p> <p class="post-meta"> 2 min read   ·   December 04, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/formatting"> <i class="fa-solid fa-hashtag fa-sm"></i> formatting</a>   <a href="/blog/tag/images"> <i class="fa-solid fa-hashtag fa-sm"></i> images</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </div> <div class="col-sm-3"> <img class="card-img" src="/assets/img/9.jpg" style="object-fit: cover; height: 90%" alt="image"> </div> </div> </li> <li> <h3> <a class="post-title" href="https://velog.io/@yeomsee97/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-From-Local-to-Global-A-Graph-RAG-Approach-to-Query-Focused-Summarization" target="_blank" rel="external nofollow noopener">[논문 리뷰] From Local to Global: A Graph RAG Approach to Query-Focused Summarization</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <h1 id="backgrounds">Backgrounds</h1> <ul> <li> <p><strong>R</strong>etrieval-<strong>A</strong>ugmented <strong>G</strong>eneartion(<strong>RAG</strong>)</p> <ul> <li>외부 지식 소스에서 질문과 관련된 정보를 검색하여 맥락으로 사용 → LLM이 이전에 보지 못한 질문에 답변할 수 있게 만듦</li> <li>지역적(local)인 데이터 분석에는 강점이 있으나, <strong>전역적(global)인 데이터 분석에는 취약</strong>함<ul> <li><em><strong>"What is the main theme of this dataset?</strong></em></li> </ul> </li> </ul> </li> <li> <p><strong>Q</strong>uery-<strong>F</strong>ocused <strong>S</strong>ummarization(<strong>QFS</strong>)</p> <ul> <li>어떤 텍스트의 단순 요약이 아닌, 질문에 대한 답변을 포함하는 요약을 생성하는 태스크</li> <li>단순히 원문에서 문장을 발췌할 수도 있지만, 질문에 맞춰 추상적으로 요약하여 자연스러운 답변을 생성할 수도 있음 (abstractive summarization)</li> <li>QFS의 경우, 대규모 데이터를 처리하는 확장성이 부족함</li> </ul> </li> </ul> <hr> <h1 id="graphrag">GraphRAG</h1> <ul> <li>핵심<ul> <li>LLM을 사용하여 그래프 기반 텍스트 인덱스를 생성<ul> <li>소스 문서에서 엔티티 지식 그래프를 도출</li> <li>밀접하게 관련된 엔티티들을 하나의 커뮤니티로 묶고, 커뮤니티에 대한 요약 생성</li> </ul> </li> <li>질문이 주어지면 커뮤니티 요약을 바탕으로 부분 응답을 생성 및 평가</li> <li>이후 모든 부분 응답이 다시 요약되어 사용자에게 최종 응답으로 제공</li> </ul> </li> </ul> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/9a135a06-f53b-4ea3-ac98-8c2fb5509cb4/image.PNG" alt=""></p> <h2 id="1-source-documents-→-text-chunks">1. Source Documents → Text Chunks</h2> <p>텍스트를 길게 자를 경우, 엔티티 추출을 위한 LLM 호출 수는 줄어들지만, 추출되는 엔티티의 수도 줄어듦 → 적절한 크기의 chunk를 사용하여 recall과 precision의 균형을 맞춰야함</p> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/08700b8f-b342-4610-8571-93baa8911eee/image.PNG" alt=""></p> <h2 id="2-text-chunks-→-element-instances">2. Text Chunks → Element Instances</h2> <ul> <li> <p><strong>LLM을 바탕으로 entity, relation, claim 추출하는 단계</strong></p> <ul> <li>entity 추출<ul> <li>name, type, description</li> </ul> </li> <li>closely-related entity들의 relation을 식별<ul> <li>source/target entity, description </li> </ul> </li> <li>few-shot 예시를 제공하여, LLM이 특정 domain에 맞게 정보를 추출하도록 조정</li> <li>보조적인 프롬프트를 바탕으로 entity와 관련된 추가적인 정보(covariate) 추출<ul> <li>subject, object, type, description, source text span, start/end dates</li> </ul> </li> <li>놓친 엔티티가 없는지 보기 위해 여러 번의 재추출(<strong>gleaning</strong>)을 거치고, 누락된 엔티티가 있다면 추가 추출을 유도해 long text chunk에서도 품질을 유지할 수 있게 함</li> </ul> <blockquote> <p><strong>Gleanings</strong> LLM에게 모든 entity가 추출됐는지 질문하여 예/아니오 중 결정하도록 설정 놓친 엔티티가 있다고 판단되면(즉, <strong>LLM의 답이 "아니오"일 경우</strong>) 다시 엔티티를 추출하는 과정을 거침</p> </blockquote> </li> </ul> <h2 id="3-element-instances-→-element-summaries">3. Element Instances → Element Summaries</h2> <ul> <li> <strong>LLM을 사용하여 소스 텍스트에서 entity, relation, claim에 대한 요약을 추출하는 단계</strong><ul> <li>각 그래프의 요소에 대해 개별적으로 생성된 요약을 하나의 설명 텍스트 블록으로 통합하는 추가적인 단계</li> </ul> </li> </ul> <h2 id="4-element-summaries-→-graph-communities">4. Element Summaries → Graph Communities</h2> <ul> <li> <strong>(여러 커뮤니티 감지 알고리즘 중) Leiden 기법을 사용하여 커뮤니티의 계층적 클러스터링이 이루어지는 단계</strong><ul> <li>Leiden 기법의 경우 대규모 그래프의 계층적 커뮤니티 구조를 효율적으로 복구할 수 있음</li> <li>entity(노드)들은 relation(엣지)로 연결<ul> <li>노드 간의 탐지된 엣지의 정규화된 개수(<em>degree</em>)로 weight 부여 → 연결이 강한 커뮤니티로 그래프를 분할</li> </ul> </li> </ul> </li> </ul> <h2 id="5-graph-communities-→-community-summaries">5. Graph Communities → Community Summaries</h2> <ul> <li> <strong>각 커뮤니티에 대한 요약을 생성하는 단계</strong><ul> <li>리프 수준 커뮤니티<ul> <li> <em>local한 질문들을 다루기 위한 과정인 듯</em> </li> <li>노드, 엣지, 공변량(covariate)에 대한 요약을 우선순위에 따라 토큰 한도에 다다를때까지 LLM의 context window에 반복적으로 추가<ul> <li>여기서 우선순위란 소스 노드와 타겟 노드의 degree를 합쳤을 때의 rank를 의미하는 듯</li> </ul> </li> </ul> </li> <li>상위 수준 커뮤니티<ul> <li> <em>global한 질문을 다루기 위한 과정인 듯</em> </li> <li>각 리프 수준 커뮤니티에 대한 요약들을 합쳤을 때 max_context_length보다 적으면 괜찮지만, 이를 넘어서는 경우 짧은 요약들로 대체하여 더 많은 주제를 다룰 수 있게 함</li> </ul> </li> </ul> </li> </ul> <p>→ 해당 과정들을 통해 대규모 데이터셋에서 전체적인 구조와 의미를 파악하는 데 도움을 줄 수 있음</p> <h2 id="6-community-summaries-→-community-answers-→-global-answer">6. Community Summaries → Community Answers → Global Answer</h2> <ul> <li> <strong>커뮤니티 요약을 이용해 최종 답변을 생성하는 단계</strong><ul> <li>Prepare Community Summaries<ul> <li>위에서 생성했던 커뮤니티 요약을 무작위로 섞고 chunking → 관련 정보가 한쪽에 치우쳐지는 것이 아니라 여러 청크에 고르게 분포하도록 함</li> </ul> </li> <li> <strong>Map</strong> community answers<ul> <li>각 청크마다 중간 답변을 병렬로 생성</li> <li>LLM이 중간 답변에 대해 0~100의 점수를 매김<ul> <li>점수가 0이라면 제외 </li> </ul> </li> </ul> </li> <li> <strong>Reduce</strong> to global answer<ul> <li>점수가 높은 순으로 중간 답변들을 정렬하고 max_context_length에 도달할 때까지 context window에 추가</li> </ul> </li> </ul> </li> </ul> <hr> <h1 id="evaluation">Evaluation</h1> <ul> <li>질의 형성 방법<ul> <li>LLM을 통해 자동으로 생성 <img src="https://velog.velcdn.com/images/yeomsee97/post/b53c9cb3-5cac-45b4-9798-5584a655e7e4/image.png" alt=""><ul> <li>LLM에게 데이터셋에 대한 설명 제공</li> <li>LLM이 잠재적 사용자 N명 및 각 사용자별 잠재적 task N개 식별하여 global question에 대한 질문 N개 생성 → N=5라면 5*5*5=125개의 테스트 데이터셋</li> </ul> </li> </ul> </li> </ul> <hr> <ul> <li>Conditions<ul> <li>CO : root-level 커뮤니티 요약 사용</li> <li>C1 : high-level 커뮤니티 요약 사용. C0의 하위 커뮤니티</li> <li>C2 : intermediate-level 커뮤니티 요약 사용. C1의 하위 커뮤니티</li> <li>C3 : low-level 커뮤니티 요약 사용. C2의 하위 커뮤니티</li> <li>TS(<strong>T</strong>ext <strong>S</strong>ummarization) : 커뮤니티 요약 대신 소스 텍스트를 Map-Reduce 요약 단계에 적용</li> <li>SS(<strong>S</strong>emantic <strong>S</strong>earch) : naive RAG 방식으로 텍스트 청크하여 context window에 추가 및 답변 생성</li> </ul> </li> </ul> <hr> <ul> <li> <p>Metrics</p> <ul> <li>Comprehensiveness : 답변이 질문의 모든 측면과 세부 사항을 잘 다루는지</li> <li>Diversity : 답변이 다양한 관점과 통찰을 제공하는지</li> <li>Empowerment : 답변이 이해와 판단에 얼마나 도움이 되는지</li> <li>Directness : 답변이 얼마나 명확하고 구체적으로 답하는지</li> </ul> <blockquote> <p><strong>평가 detail</strong> LLM을 evaluator로 설정하여 두 답변을 직접 비교하여 평가 (head-to-head 비교 방식)</p> </blockquote> </li> </ul> <hr> <ul> <li>Configuration<ul> <li>context window의 크기의 영향을 비교<ul> <li>comprehensiveness 측면에서 8k, 16k, 32k, 64k를 비교한 결과 8k에서 가장 좋은 성능을 보였기 때문에 context window의 크기를 <strong>8k</strong>로 설정</li> </ul> </li> </ul> </li> </ul> <hr> <ul> <li>Results<blockquote> <p><strong>표 해석법</strong></p> </blockquote> </li> <li> <em>행 조건이 열 조건과 비교했을 때 승률을 숫자로 나타냄*</em> : 승률은 두 조건이 125개 질문에 대해 5번씩 반복 평가된 결과로 평균을 내어 산출</li> </ul> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/69280a29-b572-4499-8f31-7d5c6c20f039/image.png" alt=""></p> <ol> <li>global approach &gt; naive RAG (Comprehensiveness, Diversity)</li> </ol> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/108c3c02-7f0d-4459-9b03-a10e162f152a/image.png" alt=""></p> <ol start="2"> <li> <p>일반적으로 커뮤니티 요약(C0, C1, C2, C3) &gt; 원본 텍스트(TS) (Comprehensiveness, Diversity)</p> </li> <li> <p>Empowerment에서는 복합적인 양상</p> </li> </ol> <p class="post-meta"> 1 min read   ·   November 03, 2024   ·   velog </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/velog"> <i class="fa-solid fa-hashtag fa-sm"></i> velog</a>   <a href="/blog/tag/technical"> <i class="fa-solid fa-hashtag fa-sm"></i> technical</a>   ·   <a href="/blog/category/external-posts"> <i class="fa-solid fa-tag fa-sm"></i> external-posts</a> </p> </li> <li> <h3> <a class="post-title" href="https://velog.io/@yeomsee97/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Leveraging-Passage-Retrieval-with-Generative-Models-for-Open-Domain-Question-Answering" target="_blank" rel="external nofollow noopener">[논문 리뷰] Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <h2 id="backgrouond">BackGrouond</h2> <ul> <li>RAG의 한계점<ul> <li>retrieve해오는 passage의 수가 늘어도 성능 개선 폭이 크지 않거나 오히려 성능이 감소하는 문제가 발생<ul> <li>이는 RAG의 decoding에서의 복잡한 aggregation 과정으로 인해 passage 수에 quadratic하게 비례하여 연산량이 증가하기 때문 (자세한 내용은 해당 <a href="https://velog.io/@yeomsee97/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks" rel="external nofollow noopener" target="_blank">포스팅</a> 참조)</li> </ul> </li> </ul> </li> </ul> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/1a1b7a56-db4d-45ad-9e98-8e1ce17055fb/image.png" alt=""></p> <hr> <h2 id="fusion-in-decoderfid">Fusion in Decoder(FiD)</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/d9eeb4b4-9080-4adf-854f-a8e02d572a60/image.png" alt=""></p> <ul> <li><strong>여러 입력 단락(문맥)을 인코더에서 개별적으로 처리한 후 디코더에서 결합하는 방식</strong></li> <li>Retriever는 학습 X<ul> <li>DPR, BM25를 사용</li> </ul> </li> </ul> <ul> <li> <p>Encoder</p> <ul> <li>passage와 query과 concat되어 독립적으로 입력됨 <strong>(process passages indepdently)</strong><ul> <li>_question, title, context_라는 special token 사용</li> </ul> </li> <li>한 번에 하나의 문맥에 대해서만 self-attention 수행<ul> <li>때문에 많은 수의 문맥으로 확장 가능함</li> <li>해당 구조는 RAG와 같음</li> </ul> </li> </ul> </li> <li> <p>Decoder</p> <ul> <li>concat된 인코더의 hidden representation들에 cross-attention 수행하여 answer 생성 <strong>(process passages jointly)</strong> </li> </ul> </li> <li> <p>RAG와는 달리 디코딩 구조가 단순하며 연산량이 linear하게 증가함</p> </li> </ul> <hr> <h2 id="experiments">Experiments</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/85e4ebd4-a538-4be8-bbfe-e09a9604bf68/image.png" alt=""></p> <ul> <li>더 많은 수의 passage를 검색해서 process하는 과정이 성능 개선에 기여</li> <li>NQ 데이터셋에서 closed book T5 모델의 경우 11B의 크기로 36.6%의 성능을 기록했지만, FiD는 770M의 pretrained T5-large 모델 + Wikipedia BM25 retrieval은 44.1%의 성능을 기록 <em>(... 아마 표에서 말하는 Fusion-in-Decoder(large)의 51.4% 기록은 retrieval 방식이 BM25가 아니라 DPR인 경우에 말하는 거 같음)</em> </li> </ul> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/722975b8-ab3b-42a2-ac95-d828c4d5701d/image.png" alt=""></p> <ul> <li> <strong># of passage ↑ - EM ↑</strong><ul> <li>많은 passage를 효과적으로 사용</li> <li>FiD 기법을 적용한 seq2seq 모델들이 다수의 passage로부터 정보를 잘 조합하는 거 같다!</li> </ul> </li> </ul> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/ddf98042-262b-4780-8b6f-2d832269b40a/image.png" alt=""></p> <ul> <li> <strong># training passage ↓ - EM ↓</strong><ul> <li>하지만, 100개의 passage로 학습하는 것은 computation cost ↑ </li> </ul> </li> <li>때문에 이전에 학습한 모델을 100개의 passage * 1000번의 step으로 fine-tuning하는 것을 제안<ul> <li>적은 수의 training passage로 학습해도 fine-tuning을 거치게 되면 100개의 passage로 학습한 것과 비슷한 성능을 갖게 됨</li> </ul> </li> </ul> <blockquote> <p><strong>Exact Match(EM)</strong> 생성된 답변이 (정규화 후) 허용 가능한 답변 목록 중 하나와 일치하면 정답으로 간주</p> </blockquote> <p class="post-meta"> 1 min read   ·   October 25, 2024   ·   velog </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/velog"> <i class="fa-solid fa-hashtag fa-sm"></i> velog</a>   <a href="/blog/tag/technical"> <i class="fa-solid fa-hashtag fa-sm"></i> technical</a>   ·   <a href="/blog/category/external-posts"> <i class="fa-solid fa-tag fa-sm"></i> external-posts</a> </p> </li> <li> <h3> <a class="post-title" href="https://velog.io/@yeomsee97/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Learning-Retrieval-Augmentation-for-Personalized-Dialogue-Generation" target="_blank" rel="external nofollow noopener">[논문 리뷰] Learning Retrieval Augmentation for Personalized Dialogue Generation</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <h2 id="prerequisite">PreRequisite</h2> <ul> <li>RAG(Retrieval-Augmented Generation)<ul> <li>retriever과 generator를 결합한 프레임워크</li> <li>질문에 답하기 위해 모델의 내부 지식만을 바탕으로 답을 하는 것이 아니라 외부 지식 소스를 이용하여 답 가능<ul> <li>hallucination ↓</li> </ul> </li> </ul> </li> <li>Personalized-dialogue Generation<ul> <li>대화형 AI 모델이 사용자의 성격, 취향, 과거 대화 등을 반영한 개인 맞춤형 응답을 생성하는 task</li> </ul> </li> </ul> <hr> <h2 id="research-background">Research Background</h2> <blockquote> <p><strong>ConvAI2 Dataset</strong></p> </blockquote> <p align="center"><img src="https://velog.velcdn.com/images/yeomsee97/post/c781370b-186c-4ac8-aa92-625efc2aeabb/image.png" width="100%"> </p> <ul> <li>personalized dialogue generation의 경우, historical context와 pre-defined persona를 바탕으로 일관된 응답의 생성을 목표로 함</li> <li>하지만, ConvAI2 데이터셋의 경우 persona를 담고 있는 문장이 4~5개밖에 안됨 → 다양하고 풍부한 답변의 생성이 어려움</li> </ul> <blockquote> <p><strong>ROCStory</strong></p> </blockquote> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/d43f01e3-7ff3-42df-8f81-134291559ea2/image.PNG" alt=""></p> <ul> <li>ROCStory는 title에 대한 story가 주어져있는 데이터셋으로 저자는 해당 데이터셋을 통해 <strong>persona profile을 풍부하게</strong> 만들 수 있을 것이라고 생각</li> <li>그럼에도 불구하고 <strong>ROCStory라는 외부지식을 어떻게 주입할 수 있을지</strong>가 문제 1) 검색해오기 위한 명시적 주석 부족 2) story 검색 성능 평가의 어려움 3) dense retriever 자체의 문제점 : dense retriever의 경우 예측된 확률분포가 기반이 됨 → 주어진 query에 대한 검색해오는 문서가 대체로 비슷하기 때문에 <strong>다양한 문서를 긁어올 수 없게 됨</strong> </li> </ul> <hr> <h2 id="model-architecture">Model Architecture</h2> <p align="center"><img src="https://velog.velcdn.com/images/yeomsee97/post/e8086af8-7b15-41f2-8fb8-87383b20ac3e/image.png" width="100%"> </p> <blockquote> <p><strong>Two-stage training process</strong> : training generator → tune the retriever + learn the retrieval augmentation</p> </blockquote> <p><strong>1. Train the Generator</strong></p> <ul> <li>generator를 supervised training<ul> <li>NLL(Negative Log-Likelihood)를 minimize하는 방향으로 generator를 supervised fine-tuning</li> <li>architecture 그림 ①에 해당함</li> </ul> </li> </ul> <p align="center"><img src="https://velog.velcdn.com/images/yeomsee97/post/ef91a1e3-7de9-4be6-85f1-6bebdf657953/image.png" width="75%"> </p> <p><strong>2. Learning Retrieval Augmentation</strong></p> <ul> <li>retriever를 tuning하는 단계<ul> <li>retriever를 업데이트하기 위해서는 retrieved content(=story)와 최종 생성된 response와의 직접적인 연결관계 필요<ul> <li>supervised generator를 evaluator로 설정</li> <li>metric은 BLUE, ROUGE-L, F1의 총합</li> </ul> </li> <li>metric 계산이 미분 불가능하기 때문에 확률 분포로 변환</li> <li>evaluator 입장에서 검색된 문서가 유용하다고 판단되면 높은 rank를 가지도록 $p_{i}$ 또한 크게 만듦</li> <li>retriever에 의해서 반환된 유사도 점수 $S_{q}$와 $P_{R}$간의 KL Divergence를 최소화하는 방향으로 학습이 진행<ul> <li>$P_{R}$ 은 metric의 확률 분포들의 값</li> </ul> </li> <li>architecture 그림 ②에 해당함</li> </ul> </li> </ul> <p align="center"><img src="https://velog.velcdn.com/images/yeomsee97/post/1444a260-321f-4dee-ad6d-96ba6b02f12a/image.png" width="100%"> </p> <p><strong>3. Retrieval Candidate Augmentation</strong></p> <ul> <li>정해진 문서 후보에만 국한되지 않도록 문서의 다양성을 높이기 위한 방법<ul> <li>확률 $ρ$에 따라 각 문서를 무작위로 대체<ul> <li>실제 학습에서는 $ρ$값이 0.5로 사용됨</li> </ul> </li> <li>대체된 문서 집합 $d_{i}^{aug}$와 쿼리의 유사도인 $S_{q}^{aug}$ 구함</li> <li>2단계에서와 마찬가지로 검색 증강 학습 적용<ul> <li>3단계는 검색 증강 학습이 이루어지는 2단계에서 jointly하게 일어남</li> </ul> </li> </ul> </li> </ul> <p align="center"><img src="https://velog.velcdn.com/images/yeomsee97/post/63f920ab-0412-4ed3-ac06-2ebd3173afb5/image.png" width="100%"> </p> <p><strong>4. Training Retrieval-Augmented Generator</strong></p> <ul> <li>1단계에서 generator를 supervised training하는 것과 큰 차이는 없음</li> <li>다만, input에 검색된 문서가 결합된다는 점이 주요한 차이점</li> <li>해당 단계 이후, retriever와 generator를 jointly하게 학습 <img src="https://velog.velcdn.com/images/yeomsee97/post/e9ff9695-6b73-482d-9046-fc56c3031454/image.png" alt=""> </li> </ul> <hr> <h2 id="experiment">Experiment</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/3d809c86-906d-4ef1-96fa-e99e47d8a2d0/image.png" alt=""></p> <ul> <li>모든 지표에서 LAPDOG을 결합한 모델이 제일 성능이 좋음</li> <li>모델 사이즈와 상관없이 일관적으로 향상됨<ul> <li>retriever : Contriever<ul> <li>BERT와 유사한 dual-encoder retriever</li> </ul> </li> <li>generator : T5</li> </ul> </li> </ul> <hr> <h2 id="ablation-studies">Ablation Studies</h2> <p align="center"><img src="https://velog.velcdn.com/images/yeomsee97/post/9220e261-d82f-438c-aca2-402ad9acf685/image.png" width="75%"> </p> <ul> <li>CandAug가 없을 경우 성능이 전반적으로 하락</li> <li>LAPDOG$_{scratch}$는 첫 번째 단계(Train the Generator) 생략한 경우 → 마찬가지로 성능이 하락</li> </ul> <p align="center"><img src="https://velog.velcdn.com/images/yeomsee97/post/cf147d3b-663b-4cb2-bd21-c7badba08712/image.png" width="75%"> </p> <ul> <li>metric에서 BLEU, F1, ROUGE-L을 각각 하나씩 제거하며 모델 성능 측정<ul> <li>BLEU가 없을 경우 : 성능이 전반적으로 다 떨어짐<ul> <li>BLEU가 응답 생성의 정확한 일치에 기여함</li> </ul> </li> <li>F1이 없을 경우 : BLEU에 비해 성능이 더 크게 떨어짐<ul> <li>F1이 생성된 응답과 실제 정답 간의 단어 중첩을 보장하는 데 기여함</li> </ul> </li> <li>ROUGE-L이 없을 경우 : 성능이 전반적으로 다 떨어짐<ul> <li>생성된 대화의 일관성을 평가함에 있어서 ROUGLE-L의 중요성 확인</li> </ul> </li> </ul> </li> </ul> <p align="center"><img src="https://velog.velcdn.com/images/yeomsee97/post/db91ed49-54cf-4a04-975c-aa6c08180044/image.png" width="75%"> </p> <ul> <li>문서 candidate의 수가 증가하면 성능이 늘어나기는 하지만 단조 증가 X</li> <li>때문에 적절한 수의 문서를 선택하는 것이 중요함<ul> <li>너무 적은 수의 candidate는 충분한 정보를 제공하지 못함</li> <li>너무 많은 수의 candidate는 무관한 정보를 제공함으로써 오히려 성능을 저하시킬 수 있음</li> </ul> </li> </ul> <p align="center"><img src="https://velog.velcdn.com/images/yeomsee97/post/1324775c-285f-4984-9859-6a4eabeda897/image.png" width="80%"> </p> <ul> <li>candidate augmentation을 하는 것이 다양한 검색을 해오는 데 도움 → 이는 더 개인화되고 문맥적으로 풍부한 답변을 생성할 수 있게끔 함</li> </ul> <p class="post-meta"> 1 min read   ·   October 18, 2024   ·   velog </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/velog"> <i class="fa-solid fa-hashtag fa-sm"></i> velog</a>   <a href="/blog/tag/technical"> <i class="fa-solid fa-hashtag fa-sm"></i> technical</a>   ·   <a href="/blog/category/external-posts"> <i class="fa-solid fa-tag fa-sm"></i> external-posts</a> </p> </li> </ul> <nav aria-label="Blog page navigation"> <ul class="pagination pagination-lg justify-content-center"> <li class="page-item "> <a class="page-link" href="/blog/page/2/" tabindex="-1" aria-disabled="2">&lt;</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/2/" title="blog - page 2">2</a> </li> <li class="page-item active"> <a class="page-link" href="/blog/page/3/" title="blog - page 3">3</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/4/" title="blog - page 4">4</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/5/" title="blog - page 5">5</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/6/" title="blog - page 6">6</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/4/">&gt;</a> </li> </ul> </nav> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Sihyeong Yeom. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> </body> </html>