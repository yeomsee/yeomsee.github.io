<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> blog | YeomSee </title> <meta name="author" content="Sihyeong Yeom"> <meta name="description" content="M.S. Candidate in AI @ Konkuk Univ. | Focus on LLM, RAG, and Efficient Reasoning "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yeomsee.github.io/blog/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> YeomSee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown active"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus <span class="sr-only">(current)</span> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item active" href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <div class="header-bar"> <h1>Sihyeong's Tech Blog</h1> <h2>논문 서베이 및 관심 연구 흐름 정리</h2> </div> <div class="tag-category-list"> <ul class="p-0 m-0"> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/blockquotes">blockquotes</a> </li> <p>•</p> <li> <i class="fa-solid fa-tag fa-sm"></i> <a href="/blog/category/external-services">external-services</a> </li> </ul> </div> <br> <div class="container featured-posts"> <div class="row row-cols-2"> <div class="col mb-4"> <a href="/blog/2021/distill/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a distill-style blog post</h3> <p class="card-text">an example of a distill-style blog post and main elements</p> <p class="post-meta"> 25 min read   ·   <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </div> </div> </div> </div> </a> </div> <div class="col mb-4"> <a href="/blog/2015/code/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a post with code</h3> <p class="card-text">an example of a blog post with some code</p> <p class="post-meta"> 4 min read   ·   <a href="/blog/2015"> <i class="fa-solid fa-calendar fa-sm"></i> 2015 </a> </p> </div> </div> </div> </div> </a> </div> </div> </div> <hr> <ul class="post-list"> <li> <h3> <a class="post-title" href="https://velog.io/@yeomsee97/Efficient-TTS-%EC%84%9C%EB%B2%A0%EC%9D%B4" target="_blank" rel="external nofollow noopener">Efficient TTS 서베이</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <p>요즘 관심 있는 연구가 Efficient Reasoning이라, 서베이했던 베이스라인 연구들을 정리 및 공유합니다.</p> <h1 id="test-time-scaling-tts">Test-Time Scaling (TTS)</h1> <p><strong>추론 시간을 늘림</strong>으로써 복잡하고 어려운 태스크에서 성능 향상 → <strong>높은 추론 시간과 비용</strong> <img src="https://velog.velcdn.com/images/yeomsee97/post/ff997acf-acc7-43fe-a2bb-0f0ed217bff4/image.png" alt=""></p> <ul> <li> <strong>최근 연구 흐름</strong> TTS 성능적 강점은 유지하면서도 불필요한 연산을 줄임으로써 효율성 개선 본 포스팅에서는 <strong>Parallel Scaling</strong>에 초점을 맞춤</li> </ul> <hr> <h1 id="ac"><a href="https://arxiv.org/pdf/2305.11860" rel="external nofollow noopener" target="_blank">AC</a></h1> <p><em>EMNLP 2023</em> <img src="https://velog.velcdn.com/images/yeomsee97/post/a5c147ee-5377-44d1-ab0b-f7b775008644/image.png" alt=""> → <strong>샘플을 하나씩 생성</strong>하며, 다수 응답의 확신이 생기면 조기 중단</p> <h1 id="esc"><a href="https://arxiv.org/pdf/2401.10480" rel="external nofollow noopener" target="_blank">ESC</a></h1> <p><em>ICLR 2024</em> <img src="https://velog.velcdn.com/images/yeomsee97/post/6ffb97a5-73fe-4355-aac5-9ff171e1edc4/image.png" alt=""> → 미리 정한 <strong>윈도우</strong>에서 모두 동일한 결과를 내면 중단</p> <h1 id="dsc"><a href="https://aclanthology.org/2025.findings-naacl.383.pdf" rel="external nofollow noopener" target="_blank">DSC</a></h1> <p><em>NAACL 2025 (Findings)</em> <img src="https://velog.velcdn.com/images/yeomsee97/post/75cc34db-3ea3-444a-ba1e-75b277b10e1f/image.png" alt=""></p> <p>AC, ESC, 그리고 그전에 리뷰했던 <a href="https://velog.io/@yeomsee97/Reasoning-Aware-Self-Consistency-Leveraging-Reasoning-Paths-forEfficient-LLM-Sampling" rel="external nofollow noopener" target="_blank">RASC</a>는 모두 <strong>일정량의 샘플링을 수행</strong>한다는 한계가 있음 → 문제의 난이도를 정하고, 그에 따라 추론 전략을 적응적으로 조절</p> <p><strong>1. 난이도 순위화</strong> 데이터를 다수의 무작위 배치로 나눠 LLM이 자체적으로 비교하며 난이도 평가 쉬운 문제부터 어려운 문제까지 정렬</p> <p><strong>2. 문제 분할</strong> 난이도 순으로 정렬 후, 쉬운 문제는 1회만 샘플링</p> <p><strong>3. 샘플 수 사전 할당</strong> 어려운 문제에 대해 필요한 샘플 수를 사전 예측하여 샘플 수 할당 이전 유사 난이도 문제의 샘플링 수를 기반으로 결정 → 입력 비용 절감</p> <h2 id="main-results">Main Results</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/3fb22cec-661f-4555-aa62-d590f2a29d02/image.png" alt=""></p> <p class="post-meta"> 1 min read   ·   January 26, 2026   ·   velog </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/velog"> <i class="fa-solid fa-hashtag fa-sm"></i> velog</a>   <a href="/blog/tag/technical"> <i class="fa-solid fa-hashtag fa-sm"></i> technical</a>   ·   <a href="/blog/category/external-posts"> <i class="fa-solid fa-tag fa-sm"></i> external-posts</a> </p> </li> <li> <h3> <a class="post-title" href="https://velog.io/@yeomsee97/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B8%B0%EB%B0%98-RAG-%EC%A0%95%EB%A6%AC-2" target="_blank" rel="external nofollow noopener">강화학습 기반 RAG 정리 (2)</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <h1 id="summary">Summary</h1> <p>Search-R1 이후, 여러 강화학습 기반 RAG 프레임워크가 등장했지만, <strong>대부분 최종 정답만을 기반으로 보상</strong>을 주는 한계 (<em>sparse reward_의 문제) **이후로는 _process-based reward</em>**를 도입하는 연구가 많이 등장</p> <hr> <h1 id="stepsearch">StepSearch</h1> <p><em>EMNLP 2025</em> <img src="https://velog.velcdn.com/images/yeomsee97/post/5af80aaf-67e7-451c-9f11-7b5c1ccd4c54/image.png" alt=""> 모델이 단순히 답만 맞히는 것이 아니라, <strong>검색 전략 자체를 잘 설계하고 실행하도록 학습 → 정교한 보상 설계 + 토큰 단위 과정 감독</strong> (보통 PPO는 에피소드 단위, 즉 출력 전체에 보상)</p> <h2 id="방법론">방법론</h2> <h3 id="data-augmentation">Data Augmentation</h3> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/ee7d0a8c-78ea-4cf9-822f-c94e8611a390/image.png" alt=""></p> <ol> <li> <p><strong>질문 분해 및 확장</strong> GPT-4o를 활용하여 MusiQue 데이터의 복합 질문을 하위 질문-답변 쌍으로 변환</p> </li> <li> <p><strong>검색 쿼리 생성</strong><br>각 하위 질문을 기반으로 N개의 검색 쿼리를 만들어 정보 검색에 활용 → 서로 다른 표현이나 키워드를 사용하여 검색 다양성을 확보할 수 있음</p> </li> <li> <p><strong>검색 및 필터링</strong> 생성된 질문을 바탕으로 M개의 검색 엔진으로 검색 실행 → 절반 이상의 검색 엔진으로부터 유효한 결과가 나오는 검색 쿼리만 유지</p> </li> </ol> <h3 id="train-llm-with-search-actions">Train LLM with Search Actions</h3> <ol> <li> <p><strong>프롬프트 설계</strong> → <strong>생각-검색-정보-답변</strong>이라는 일관된 절차를 따르게 함 <img src="https://velog.velcdn.com/images/yeomsee97/post/02a9a2cb-78a8-47cb-b6fb-ed401ec41adc/image.png" alt=""></p> </li> <li> <p><strong>롤아웃 과정</strong></p> </li> </ol> <ul> <li>&lt;think&gt;&lt;/think&gt;: 내부 추론 단계</li> <li>&lt;search&gt;&lt;/search&gt;: 검색 쿼리 생성</li> <li>&lt;information&gt;&lt;/information&gt;: 검색 결과 입력</li> <li>LLM이 &lt;answer&gt;&lt;/answer&gt;를 출력하거나, 액션 예산이 소진되면 중단</li> </ul> <ol start="3"> <li> <strong>마스킹을 통한 학습 집중</strong> 늘 그렇듯, &lt;information&gt;&lt;/information&gt; 구간은 그레디언트 계산에서 제외</li> </ol> <h3 id="보상-설계">보상 설계</h3> <ol> <li> <p><strong>Type 1 Reward: Global Signal</strong></p> <p>$r_{\text{overall}} = r_{\text{answer}} + \gamma_{\text{key}} \cdot r_{\text{key}}$</p> <ul> <li> <strong>형식 요구사항</strong><ul> <li>&lt;search&gt;&lt;/search&gt; 쌍 안의 검색 쿼리만 추출되어 검색 도구 호출에 사용되며, 정답은 &lt;answer&gt;&lt;/answer&gt; 쌍 안에 위치해야 함</li> <li>최소 한 번 이상의 think 및 search 동작을 포함해야 함</li> <li>질문에 대한 답변은 오직 하나의 &lt;answer&gt;&lt;/answer&gt; 태그 쌍으로 작성하며, 반드시 마지막에 위치</li> </ul> </li> </ul> </li> </ol> <ul> <li> <p><strong>Answer Reward</strong> </p> <ul> <li> <p>$F1(x, y) = \frac{2 * I_N}{P_N + R_N}$</p> </li> <li> <p>$r_{\text{answer}} = \begin{cases} F1(a_{\text{pred}}, a_{\text{gt}}), &amp; \text{format is correct}, \ 0, &amp; \text{format is incorrect}. \end{cases}$</p> </li> <li> <p><strong>Search Keys Reward</strong></p> <ul> <li>모델이 만든 쿼리와 아까 만들었던 검색 쿼리들과의 F1 점수 계산</li> <li>가장 높은 F1 점수를 선택 (하위 질문에 대해 모델이 만든 최고의 쿼리 품질)</li> <li>롤아웃을 하며 만들어진 모든 하위 질문에 대해 평균 계산하여 보상 $r_{key}$로 줌</li> </ul> </li> </ul> </li> </ul> <ol start="2"> <li> <p><strong>Type 2 Reward: Search Step</strong></p> <p>$r^{t}_{\text{step}} = G_t - P_t$ (정보 이득 - 중복 패널티)</p> <ul> <li><strong>정보 이득</strong></li> </ul> <p>$G^t = \frac{1}{n} \sum_{i=1}^n \max(c^t_i - m^t_i, 0)$ </p> <ul> <li>$k$개의 문서 검색</li> <li>각 골드 문서와 검색된 문서들의 코사인 유사도 계산</li> <li>이번 턴에서 가장 유사한 문서를 찾고, 이전 턴까지의 최고 유사도와 비교</li> <li>이전보다 더 높아진 부분만 정보 이득으로 기록</li> </ul> <ul> <li> <p><strong>중복 패널티</strong></p> <p>$P^t = \frac{1}{k} \sum_{j=1}^{k} \mathbb{1}(d^{r(t)}_j \in H^{t-1})$</p> <ul> <li>이전에 본 문서를 또 가져오면 그만큼 점수를 깎아서, 모델이 새로운 정보를 찾게 만듦</li> </ul> </li> </ul> </li> </ol> <h2 id="main-results">Main Results</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/d46c123c-3d3e-4787-ba5a-21999e3b5bca/image.png" alt=""></p> <hr> <h1 id="r3-rag">R3-RAG</h1> <p><em>EMNLP 2025 (Findings)</em> <img src="https://velog.velcdn.com/images/yeomsee97/post/c4dcb902-b62c-46c4-9ef0-12ecbe935ece/image.png" alt=""></p> <h2 id="방법론-1">방법론</h2> <p><strong>단계별 학습</strong></p> <ol> <li> <p><strong>Cold Start (SFT)</strong> 모델이 <strong>단계별 추론 + 검색</strong>을 하는 습관을 먼저 가르치는 예열 단계 질문 하나당, 추론과 검색이 오가는 과정 + 예측 정답이 세트로 기록되며, <strong>틀린 정답으로 끝나는 경로는 제거</strong> → 잘못된 추론 습관이 학습 데이터에 들어가지 않게 하기 위함</p> </li> <li> <p><strong>RL</strong> 하지만, Cold Start는 모델이 외부 검색 환경을 충분히 탐색하기 어렵기 하므로, <strong>강화학습</strong>을 통해 관련성 높은 외부 지식을 더 잘 검색하는 능력 강화</p> </li> </ol> <p><strong>보상 방식</strong></p> <p><strong><em>Answer Correctness + Document Relevance + Format Correctness</em></strong></p> <ol> <li> <p><strong><em>Answer Correctness</em></strong></p> <p> $Acc(a) = \begin{cases} 1 &amp; \text{if } Acc_{match}(a) \text{ or } Acc_{model}(a) \ 0 &amp; \text{otherwise} \end{cases}$ → 모델의 추론 경로가 정답에 도달했는지 평가하는 <strong>결과 보상 함수</strong> (단순 문자열 매칭 외에도 모델 기반 판단도 도입)</p> </li> <li> <p><strong><em>Document Relevance</em></strong> $\text{Rel}(d_i^j) = \text{LLM}(I_{\text{relevance}}, q_i, d_i^j)$ → 모델이 사용자 질문과 관련성 높은 문서를 검색하도록 유도하는 <strong>과정 보상 함수</strong></p> </li> <li> <p><strong><em>Format Correctness</em></strong> 늘 그렇듯, 포맷 맞았냐 안 맞았냐로 점수 준다고 생각</p> </li> <li> <p><strong>과정 보상 조정</strong> $r_{\text{process}}^*(s_i^j) = r_{\text{process}}(s_i^j) \cdot \lambda_T(T_i)$</p> <p>→ 다른 보상들로 인해 정답 정확도 기반 보상의 효과가 약해지지 않도록 함</p> <ul> <li> <p><strong>정답이 올바른 경로의 경우</strong>: $\lambda_T(T_i)&gt;0$</p> </li> <li> <p><strong>정답이 틀리거나, 과도한 사고로 답변 생성에 실패한 경우</strong>: $\lambda_T(T_i)&lt;0$</p> </li> <li> <p><strong>형식 오류가 있는 경로의 경우</strong>: $\lambda_T(T_i)=1$ → 형식 오류는 특정 스텝에서만 발생하는 단발성 문제이기 때문에, 경로 전체에 영향을 주지 않기 위함</p> </li> </ul> </li> </ol> <h2 id="main-results-1">Main Results</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/f996f0a7-6ff4-493d-bfcf-b95b7ea0a78e/image.png" alt=""></p> <hr> <h1 id="autorefine">AutoRefine</h1> <p>_NeurIPS 2025 _ <img src="https://velog.velcdn.com/images/yeomsee97/post/aae682b8-7cac-44e4-95f1-b359f44a3dce/image.png" alt=""></p> <p><strong><em>search-and-refine-during-think</em></strong> 패러다임을 활용한 강화학습 기반 RAG 프레임워크</p> <ol> <li>&lt;refine&gt;&lt;/refine&gt;: 검색된 문서의 핵심적인 정보 추출 및 무관한 정보 제외</li> <li>결과 기반 보상과 검색 특화 보상을 결합한 공동 리워드 설계 + GRPO 학습 → 모델이 추론 전반에 걸쳐 세밀한 지식을 추출, 조직, 활용할 수 있게 도와줌</li> </ol> <h2 id="방법론-2">방법론</h2> <h3 id="rollout-generation">Rollout Generation</h3> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/55d719e8-4b61-4f2e-9194-6a07787cc207/image.png" alt=""></p> <ul> <li>&lt;think&gt;&lt;/think&gt;: 검색 행동에 대한 전반적인 계획</li> <li>&lt;search&gt;&lt;/search&gt;: 검색 쿼리 생성</li> <li>&lt;document&gt;&lt;/document&gt;: 검색된 문서 반환</li> <li>&lt;refine&gt;&lt;/refine&gt;: 검색된 내용에서 관련 정보 추출</li> <li>&lt;answer&gt;&lt;/answer&gt;: 최종 답변 생성</li> </ul> <h3 id="보상-함수-설계">보상 함수 설계</h3> <p><strong>결과 기반 보상</strong></p> <p>$R_{\text{Ans}} = F1(o_{\text{ans}}, a) = \frac{2|o_{\text{ans}} \cap a|}{|o_{\text{ans}}| + |a|}$ 흔히 볼 수 있는 생성 답변과 정답과의 F1 점수</p> <p><strong>검색 특화 보상</strong></p> <p>$R_{\text{Ret}} = I(a \cap o_{\text{refine}} = a)$</p> <ul> <li>$o_\text{refine}$: 모든 <refine></refine> 블록 내 문서를 수집하여 하나의 텍스트 시퀀스로 만듦</li> </ul> <p>즉, 정답 $a$에 포함된 모든 요소(핵심 단어, 문구 등)가 $o_\text{refine}$에 모두 들어있으면 1, 아니면 0</p> <p><strong>최종 보상 형태</strong> $R_{\text{Overall}} = \begin{cases} R_{\text{Ans}}, &amp; \text{if } R_{\text{Ans}} &gt; 0 \ 0.1, &amp; \text{if } R_{\text{Ans}} = 0 \text{ and } R_{\text{Ret}} &gt; 0 \ 0, &amp; \text{if } R_{\text{Ans}} = R_{\text{Ret}} = 0 \end{cases}$</p> <h2 id="main-results-2">Main Results</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/abaddd71-03e7-4291-ac50-b01f8d5a2d57/image.png" alt=""></p> <p class="post-meta"> 1 min read   ·   January 26, 2026   ·   velog </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/velog"> <i class="fa-solid fa-hashtag fa-sm"></i> velog</a>   <a href="/blog/tag/technical"> <i class="fa-solid fa-hashtag fa-sm"></i> technical</a>   ·   <a href="/blog/category/external-posts"> <i class="fa-solid fa-tag fa-sm"></i> external-posts</a> </p> </li> <li> <h3> <a class="post-title" href="https://velog.io/@yeomsee97/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B8%B0%EB%B0%98-RAG-%EC%A0%95%EB%A6%AC" target="_blank" rel="external nofollow noopener">강화학습 기반 RAG 정리 (1)</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <h1 id="search-r1">Search-R1</h1> <blockquote> <p>COLM 2025 논문 <a href="https://arxiv.org/pdf/2503.09516" rel="external nofollow noopener" target="_blank">링크</a></p> </blockquote> <p><strong>DeepSeek-R1</strong>의 확장 모델로, 강화학습만을 통해 LLM이 단계적으로 reasoning을 수행하면서 다수의 검색 쿼리를 자율적으로 생성하고, 실시간 검색 결과를 활용할 수 있는 프레임워크</p> <h2 id="배경">배경</h2> <p><em>Search-and-Reasoning</em> 시나리오에 강화학습을 적용할 때 다음과 같은 도전들이 존재</p> <ol> <li> <p><strong><em>RL Framework and Stability</em></strong> 강화학습이 안정적으로 최적화될 수 있는가?</p> </li> <li> <p><strong><em>Multi-Turn Interleaved Reasoning and Search</em></strong> multi-turn 상황에서 어떻게 동적으로 검색 전략을 수정 및 수행할 것인가?</p> </li> <li> <p><strong><em>Reward Design</em></strong> 보상 함수를 어떻게 효과적으로 설계할 것인가?</p> </li> </ol> <h2 id="방법">방법</h2> <ol> <li> <p><strong>검색 엔진</strong>을 <strong>환경</strong>의 일부로 모델링 <strong>검색 토큰</strong>을 <strong>마스킹</strong>함으로써 강화학습 훈련을 최적화</p> <p> (PPO, GRPO의 경우, 기본적으로 LLM에서는 토큰 별로 loss가 계산된다고 함. 그런데, LLM이 생성하지 않은 검색 결과에 대해 loss를 적용하면 의도치 않게 학습이 수행될 수 있어 검색 결과에 대해서는 loss masking을 통해 loss를 계산하지 않는 듯)</p> </li> <li> <p><strong><em>multi-turn retrieval/reasoning</em></strong></p> <ul> <li>*&lt;search&gt;&lt;/search&gt;*: 검색 호출</li> <li>*&lt;information&gt;&lt;/information&gt;*: 검색 결과 삽입</li> <li>*&lt;think&gt;&lt;/think&gt;*: LLM의 추론 과정</li> <li>*&lt;answer&gt;&lt;/answer&gt;*: 최종 정답</li> </ul> </li> <li> <p><em>process-based reward</em>가 아닌, <strong><em>outcome-based reward</em></strong> 사용</p> </li> </ol> <p><strong>최소한의 보상 설계</strong>를 바탕으로 검색 기반 추론 시나리오에서 효과적임을 보여줌 (실제로 <a href="https://arxiv.org/pdf/2501.12948" rel="external nofollow noopener" target="_blank">최근 연구</a>에서는 <strong>강화학습을 통해 LLM이 결과 기반 보상만으로도 고도화된 추론 능력을 학습</strong>할 수 있음을 보여주었지만, <strong>이러한 접근 방식이 검색 엔진 호출 시나리오에 적용될 수 있는 가능성은 아직 충분히 탐구되지 않음</strong>)</p> <blockquote> <p><strong><em>Reward Modeling</em></strong></p> </blockquote> <p>$r_\phi(x, y) = \mathrm{EM}(a_{\text{pred}}, a_{\text{gold}})$</p> <p><em>Deepseek-R1</em>에서 format reward를 주었지만, 본 논문에서는 주지 않았음 → 모델이 이미 구조적인 형식을 충분히 잘 따르고 있기 때문에, 보다 복잡한 형식 보상의 탐색은 향후 과제로 남겨둠</p> <blockquote> <p><strong>작동 과정</strong></p> </blockquote> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/796735e6-ccfc-4c79-8228-76b51f16652b/image.png" alt=""></p> <h2 id="main-results">Main Results</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/822db0b6-5b6a-461c-8dbf-536f31764a59/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/358b9f70-8d8c-428e-8d99-04d73296438a/image.png" alt=""></p> <ol> <li><strong>GRPO &gt; PPO</strong></li> <li> <strong>Instruct Model에 강화학습을 적용</strong>하는 것이 그냥 base model을 훈련하는 것보다 <strong>더 빨리 최적화</strong>됨 (하지만, 최종 성능은 유사함)</li> <li> <strong>검색 결과에 마스킹을 적용</strong>한 것이 그렇지 않은 것보다 성능이 좋음</li> </ol> <hr> <h1 id="r1-searcher">R1-Searcher</h1> <blockquote> <p>arXiv 2025.03.07 논문 <a href="https://arxiv.org/pdf/2503.05592" rel="external nofollow noopener" target="_blank">링크</a></p> </blockquote> <p><strong>훈련 과정 중 외부 검색 환경과 직접 상호작용</strong>하도록 하여, LLM이 <strong>검색을 학습하고 활용</strong>할 수 있도록 <strong>2단계 결과 기반 강화학습 방식</strong>을 적용한 프레임워크</p> <p>해당 프레임워크가 적용한 <strong>2단계 결과 기반 강화학습</strong> 방식은 다음과 같음:</p> <ul> <li> <p><strong>1단계: 검색 보상</strong> 사용 최종 정답의 정확도는 고려하지 않고, <strong>검색 행위를 유도</strong>하기 위한 보상 사용</p> </li> <li> <p><strong>2단계: 정답 보상</strong> 사용 모델이 외부 검색 시스템을 효과적으로 활용하여 <strong>문제를 정확히 해결</strong>하는 능력 학습</p> </li> </ul> <h2 id="배경-1">배경</h2> <p><strong>기존 LRM의 한계</strong></p> <p>강화학습을 통해 언어 모델의 추론 능력 향상에 기여했지만, 주로 <strong>내부 지식에 의존</strong>하기 때문에, <strong>시의성</strong>이 요구되거나 <strong>지식 집약적</strong>인 질문을 마주치는 상황에는 <strong>매우 취약함</strong></p> <p><strong>기존 RAG 연구의 한계</strong></p> <ul> <li> <strong>프롬프트 기반 접근</strong>: 주로 폐쇄형 모델에 의존해 <strong>실용성이 떨어짐</strong> </li> <li> <strong>SFT 기반 증류</strong>: 해결 경로를 암기하게 만들어 <strong>일반화 성능이 떨어짐</strong> </li> <li> <strong>MCTS 기반 추론 확장 방식</strong>: 효과적이지만, 추론 속도 저하로 인해 <strong>실제 적용이 어려움</strong> </li> </ul> <h2 id="방법-1">방법</h2> <h3 id="data-selection">Data Selection</h3> <p>검색 환경은 독립적이기 때문에, 모든 질문에 대한 적절한 정보를 포함하고 있지 않을 수 있음 → <strong>롤아웃 횟수</strong>, 즉 난이도에 따라 <strong>검색 시스템이 처리 가능한 질문을 선별</strong> <img src="https://velog.velcdn.com/images/yeomsee97/post/19233cda-90db-4e48-b202-4cd2cc4a72aa/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/f9845921-f8a4-46af-b633-2d28b8b87f93/image.png" alt=""></p> <h3 id="two-stage-outcome-based-reinforcement-learning">Two-Stage Outcome-based Reinforcement Learning</h3> <h4 id="reward-design"><strong>Reward Design</strong></h4> <p><strong>1단계: Retrieve Reward</strong></p> <p><strong>검색하는 방법</strong>부터 익히자!</p> <p>$R_{retrieval} = \begin{cases} 0.5, &amp; n \geq 1 \ 0, &amp; n = 0 \end{cases}$</p> <p>여기서 $n$은 검색 호출 횟수</p> <p>$R_{format} = \begin{cases} 0.5, &amp; \text{if the format is correct} \ 0, &amp; \text{if the format is incorrect} \end{cases}$</p> <p><strong>2단계: Answer Reward</strong></p> <p>검색하는 방법 익혔으니, 이걸 바탕으로 <strong>문제 해결 능력</strong>을 키워보자!</p> <p>$R'_{format} = \begin{cases} 0, &amp; \text{if the format is correct} \ -2, &amp; \text{if the format is incorrect} \end{cases}$</p> <p>$R_{answer} = \frac{2 \cdot IN}{PN + RN}$</p> <p>정답과 예측의 F1 점수라고 생각하면 됨</p> <ul> <li>$PN$: 예측 답변의 단어 수</li> <li>$RN$: 정답 답변의 단어 수</li> <li>$IN$: 정답 답변과 예측 답변의 공통되는 단어 수</li> </ul> <h4 id="training-algorithm">Training Algorithm</h4> <p><a href="https://arxiv.org/pdf/2501.03262" rel="external nofollow noopener" target="_blank">Reinforce++ 알고리즘</a> 기반 RAG 시나리오에 해당 알고리즘을 맞춤 적용하기 위해, 아래와 같은 두 가지 변형 방법들을 적용</p> <ol> <li><strong>RAG-based Rollout</strong></li> </ol> <p>*<end_of_query><em>를 생성하면, 생성이 일시 중단 *<begin_of_query><end_of_query></end_of_query></begin_of_query></em> 태그를 사용하여 검색 도구 호출을 명시</end_of_query></p> <p> 검색된 문서는 *<begin_of_documents><end_of_documents>* 태그로 감싸져 모델의 추론 과정에 통합 <img src="https://velog.velcdn.com/images/yeomsee97/post/ad99b0d2-bd09-4e86-885f-e6b8a87cbec6/image.png" alt=""></end_of_documents></begin_of_documents></p> <ol start="2"> <li><strong>Retrieval Mask-based Loss Calculation</strong></li> </ol> <p>*<begin_of_documents><end_of_documents>* 에 해당하는 부분은 학습하는 동안 마스킹 → 환경 효과를 줄이기 위함</end_of_documents></begin_of_documents></p> <h2 id="main-results-1">Main Results</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/7fa06bd4-715e-4b81-a268-e91194145ee6/image.png" alt=""></p> <ol> <li>SFT 없이 강화학습만으로 <strong>멀티홉 QA에서 좋은 성능</strong> </li> <li>좋은 <strong>일반화</strong> 성능 → 학습은 HotpotQA, 2Wiki에서 추출한 8천여개의 데이터로 진행했음에도, 다른 벤치마크 데이터셋에서도 좋은 성능을 보여주고 있음</li> </ol> <hr> <h1 id="research">ReSearch</h1> <blockquote> <p>arXiv 2025.03.25 논문 <a href="https://arxiv.org/pdf/2503.19470" rel="external nofollow noopener" target="_blank">링크</a></p> </blockquote> <p><strong>강화학습</strong>을 통해 <strong>추론과 검색 간 상호작용</strong>을 학습 추론 단계에 대한 지도 학습 데이터 없이도 가능</p> <h2 id="연구-배경">연구 배경</h2> <p><em>Deepseek-R1</em>과 같이, 규칙 기반 보상 함수가 LLM의 정교한 추론 패턴을 자율적으로 발전시키는 데 효과적으로 작용할 수 있음을 보여주었지만, 현재 접근 방식은 <strong>내부 추론 능력 향상에 초점</strong>을 맞추고 있으며, 이 <strong>추론 과정을 외부 지식 검색과 효과적으로 결합하는 방법에 대한 탐색은 제한적</strong></p> <h2 id="방법-2">방법</h2> <h3 id="reinforcement-learning">Reinforcement Learning</h3> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/e258e29b-a5de-42a2-a9ab-4722e7f9edb6/image.png" alt=""> (a)와 (b)는 GRPO 파이프라인, 롤아웃 생성 과정을 각각 보여줌</p> <p>여기서, *&lt;search&gt;&lt;/search&gt;<em>: 검색 쿼리 *&lt;result&gt;&lt;/result&gt;</em>: 검색 결과 → eos 토큰이 나올 때까지 concat</p> <h3 id="training-template">Training Template</h3> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/3ebd13c8-77b8-4bfb-82df-571397f0ec18/image.png" alt=""></p> <h3 id="reward-modeling">Reward Modeling</h3> <p>$r = \begin{cases}\mathrm{f1}(a_{\text{pred}}, a_{\text{gt}}), &amp; \text{if f1 score is not 0} \ 0.1, &amp; \text{if f1 score is 0 and format is correct} \ 0, &amp; \text{if f1 score is 0 and format is incorrect}\end{cases}$</p> <h2 id="experiments">Experiments</h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/bc38c6ed-b723-47bd-8265-f7304c52e1b1/image.png" alt=""></p> <ol> <li>ReSearch가 <strong>전반적으로 모든 벤치마크에서 성능 향상</strong>을 보임 → 심지어, <strong>MuSiQue 데이터셋에서만 학습이 진행</strong>됐지만, 다른 벤치마크에서도 성능이 좋았음</li> <li><strong>베이스 모델보다는 instruct 모델이 성능이 더 좋음</strong></li> </ol> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/9e1355ee-9db1-4a99-bc14-4d1f69c138ec/image.png" alt=""> → 학습 단계에서의 응답 횟수 변화</p> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/a79adc1e-9651-4aec-84c2-aa4709905c2d/image.png" alt=""> → 학습 및 검증 단계에서의 보상 변화</p> <hr> <h1 id="zerosearch-incentivize-the-search-capability-of-llms-without-searching">ZeroSearch: Incentivize the Search Capability of LLMs without Searching</h1> <blockquote> <p>arXiv 2025.05.07 논문 <a href="https://arxiv.org/pdf/2505.04588" rel="external nofollow noopener" target="_blank">링크</a></p> </blockquote> <p><strong>훈련 중 실제 검색 엔진과의 상호작용 없이도, LLM이 검색 엔진을 활용하는 능력을 유도</strong>할 수 있는 프레임워크</p> <h2 id="문제">문제</h2> <p>최근 연구들은 강화학습을 활용하여 LLM의 검색 능력을 향상 하지만,</p> <ol> <li> <p><strong>통제 불가능한 문서 품질</strong> 검색 엔진이 반환하는 문서에 <strong>노이즈</strong>가 낄 수도 있어 문서의 품질이 예측 불가능 → 학습 과정에 불안정성 유발</p> </li> <li> <p><strong>과도하게 높은 API 비용</strong> 강화학습 훈련은 <strong>반복적인 롤아웃</strong>을 필요로 하며, 이는 수십만 건에 이르는 검색 요청 수반 가능 → 막대한 API 비용 초래 및 확장성 제한</p> </li> </ol> <h2 id="방법-3">방법</h2> <h3 id="reinforcement-learning-without-a-search-engine">Reinforcement Learning without a Search Engine</h3> <p>$\text{max}<em>{π_\theta} 𝔼</em>{x∼D, y∼π<em>\theta(·|x; π_ψ)} [ r_\phi(x, y) ] − βD</em>{\text{KL}}[(\pi_\theta(y | x; π_\phi) || π_\text{ref}(y | x; π_ψ) ]$</p> <ul> <li>$\pi_{\theta}$: 정책 모델</li> <li>$\pi_\text{ref}$: 참조 모델</li> <li>$\pi_\psi$: 시뮬레이션 LLM(학습 중에는 고정된 상태로 유지됨)</li> <li>$r_\phi$: 보상 함수</li> </ul> <h3 id="training-template-1">Training Template</h3> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/bf45c80e-69e3-4c7c-837a-4a37783644d7/image.png" alt=""></p> <h3 id="search-simulation-tuning">Search Simulation Tuning</h3> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/e158fad6-63da-4505-8fff-ecfde83a7bcf/image.png" alt=""> <strong>LLM에게 검색 결과처럼 쓰도록 지도학습</strong></p> <ol> <li> <p>실제 검색 엔진 사용해서 <strong>학습 데이터 수집</strong> LLM이 실제 검색 엔진을 통해 여러 번 검색하는 멀티턴 상호작용 로그 수집 예: 쿼리 → 검색 → 문서 → 답변</p> </li> <li> <p><strong>질의-문서 쌍 추출</strong> 로그에서 질의와 거기에 대해 검색된 문서들을 분리해서 쌍을 만듦</p> </li> <li> <p><strong>문서가 쿼리에 유용했는지 판단</strong> LLM을 판별자로 써서, 각 문서가 질의에 답할 수 있을 정도로 충분한 정보를 담고 있는지 평가/라벨링</p> <ul> <li> <strong>답변 가능 →</strong> <em>useful</em> </li> <li> <strong>답변 불가능 →</strong> <em>noisy</em> </li> </ul> </li> <li> <p><strong>경량 지도 학습 (SFT)</strong> 수집된 데이터를 바탕으로 LLM 학습</p> </li> </ol> <h3 id="rollout-with-curriculum-search-simulation">Rollout with Curriculum Search Simulation</h3> <p><strong>훈련 난이도를 점진적으로 높이기 위해 생성되는 문서의 품질을 시간에 따라 점차 저하</strong></p> <p>구체적으로, 확률 함수 $p_i$를 활용하여 노이즈 문서가 생성될 확률 조절</p> <p>$p_i = p_s + \frac{b^{i/m}-1}{b-1}(p_e - p_s)$</p> <ul> <li>$p_s$: 초기 노이즈 확률</li> <li>$p_e$: 최종 노이즈 확률</li> <li>$i$: 현재 훈련 단계 수</li> <li>$m$: 전체 훈련 단계 수</li> <li>$b$: 기본적으로 4를 줌</li> </ul> <p>즉, 훈련이 진행되면서 $i/m$ 비율이 증가하고, 이는 $p_i$의 증가를 야기하며, 이는 곧 노이즈 문서가 생성될 확률을 증가시킴</p> <p>→ <strong>정책 모델</strong>이 <strong>처음</strong>에는 <strong>기본 출력 형식과 작업 요구 사항 학습</strong> → <strong>점차 더 어렵고, 노이즈가 더 많은 검색 상황에 적응</strong>할 수 있게 됨</p> <h3 id="reward-design-1">Reward Design</h3> <ol> <li> <strong>정답 정확도만을 중점</strong>으로 둔 <strong>규칙 기반 보상 함수</strong> 설정 (<strong>EM</strong>의 경우, 정책 모델이 정답을 포함할 확률을 높이기 위해 불필요하게 긴 답변을 생성하는 경향을 보이는 일종의 <strong>리워드 해킹</strong> 발생)<ul> <li>$r_\phi(x, y) = \frac{2 \times IN}{PN + RN}$<ul> <li>즉, 그냥 F1 점수라고 생각하면 됨 <ul> <li>$IN$: 예측과 정답 사이의 겹치는 단어 수</li> <li>$PN$: 예측 응답 내 단어 수</li> <li>$RN$: 정답 응답 내 단어 수</li> </ul> </li> </ul> </li> </ul> </li> <li>출력 형식에 대한 보상은 부여하지 않았음 (별도의 감독 없이도 모델이 일관되게 잘 생성) → 논문 읽다 보니, 확실한 건 Qwen 모델 계열이 별도의 감독 없이도 그냥 잘 생성한다는 느낌을 받음</li> </ol> <h3 id="training-algorithm-1">Training Algorithm</h3> <p>문서 토큰은 마스킹을 함으로써, 정책 모델이 직접 생성한 토큰에 대해서만 그레디언트 계산</p> <h2 id="main-results-2">Main Results</h2> <p><strong>Base Model</strong></p> <ul> <li><em>Qwen-2.5-7B, Qwen-2.5-3B(Base/Instruct)</em></li> <li><em>LLaMA-3.2-3B</em></li> </ul> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/c2767914-fb44-4e3e-995f-da2f8d0f04a8/image.png" alt=""></p> <hr> <h1 id="느낀-점">느낀 점</h1> <ul> <li> <strong>결과 기반</strong>으로 보상을 1~2개로 단순하게 줌<ul> <li>규칙 기반 보상 함수가 주를 이룸<ul> <li>이는 <em>DeepSeek-R1</em> 때문일 것으로 생각됨</li> <li>하지만, 과정 기반으로 보상을 주는 RAG 프레임워크도 있을텐데,, 서베이가 필요함</li> </ul> </li> </ul> </li> <li>생각보다 학습 데이터의 양이 많지 않음<ul> <li> <strong>R1-Searcher</strong>: HotpotQA, 2wikimultihopqa에서만 학습했는데도, 다른 벤치마크에서도 성능이 일괄적으로 상승</li> <li> <strong><em>Re</em>Search</strong>: MusiQue 데이터셋에서만 학습했는데도, 다른 벤치마크에서도 성능이 일괄적으로 상승</li> </ul> </li> </ul> <p class="post-meta"> 1 min read   ·   July 25, 2025   ·   velog </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/velog"> <i class="fa-solid fa-hashtag fa-sm"></i> velog</a>   <a href="/blog/tag/technical"> <i class="fa-solid fa-hashtag fa-sm"></i> technical</a>   ·   <a href="/blog/category/external-posts"> <i class="fa-solid fa-tag fa-sm"></i> external-posts</a> </p> </li> <li> <h3> <a class="post-title" href="https://velog.io/@yeomsee97/Reasoning-Aware-Self-Consistency-Leveraging-Reasoning-Paths-forEfficient-LLM-Sampling" target="_blank" rel="external nofollow noopener">[논문 리뷰] Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <h1 id="summary">Summary</h1> <h2 id="self-consistency">Self-Consistency</h2> <p>LLM이 여러 개의 추론 경로를 만들고 <strong>가장 자주 등장한 정답을 고르는 방식</strong></p> <ol> <li>단순히 결과가 같다는 것에 의존할 뿐, <strong>“왜 그 답을 골랐는지”에 대한 추론의 질</strong>은 고려 X</li> <li> <strong>몇 개의 샘플을 생성해야 적절한지 판단하는 기준도 없음</strong>. 무작정 20~40개 만드는 경우가 많음</li> </ol> <hr> <h2 id="rascreasoning-aware-self-consistency"><strong>RASC(Reasoning Aware Self-Consistency)</strong></h2> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/499b9aa7-a32c-42b9-8933-5d672771deba/image.png" alt=""> LLM이 답을 낼 때, <strong>더 적은 샘플로 더 믿을 수 있는 추론을 고르게</strong> 도와주는 방법</p> <p><strong>핵심</strong> 각 샘플의 출력 <strong>결과</strong>와 추론 <strong>과정</strong> <strong>둘 다 평가</strong> 단순히 정답만 보는 것이 아닌, <strong>그 답에 도달한 논리적 과정이 믿을 만한지</strong>까지 따짐</p> <p><strong>방법</strong></p> <ol> <li>LLM으로부터 reasoning-answer pair <strong>샘플</strong> 생성</li> <li>샘플로부터 <strong>feature vector</strong> 추출 (<strong>rule-based</strong>) <img src="https://velog.velcdn.com/images/yeomsee97/post/2b74b953-8b72-4420-9b6e-1afa571d70e8/image.png" alt="table1"> </li> <li>학습된 classifier로부터 <strong>sufficient score (0~1 사이)</strong> 계산</li> <li>sufficient score가 <strong>임계값 $T$을 넘으면 버퍼에 저장</strong> </li> <li>버퍼의 크기가 <strong>임계값 $N$에 도달</strong>하면, sampling을 <strong>조기 종료</strong> </li> <li>버퍼에 저장된 것 중, 정답 별로 점수 총합을 계산하여 <strong>가장 점수 총합이 높은 답을 최종 선택</strong> </li> <li>최종 정답을 선택한 샘플 중, <strong>가장 높은 sufficient score를 가진 sample을 최종 reasoning-answer pair로 선택</strong> </li> </ol> <hr> <h1 id="result">Result</h1> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/0bc97871-15c9-43d5-9663-8b73ce3dac23/image.png" alt="table2"> → <strong>Self-Consistency에 비해 적게 생성하면서도, 성능은 그에 준하는 모습을 보임</strong></p> <p class="post-meta"> 1 min read   ·   May 15, 2025   ·   velog </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/velog"> <i class="fa-solid fa-hashtag fa-sm"></i> velog</a>   <a href="/blog/tag/technical"> <i class="fa-solid fa-hashtag fa-sm"></i> technical</a>   ·   <a href="/blog/category/external-posts"> <i class="fa-solid fa-tag fa-sm"></i> external-posts</a> </p> </li> <li> <h3> <a class="post-title" href="https://velog.io/@yeomsee97/DRAGIN-Dynamic-Retrieval-Augmented-Generation-based-on-the-Information-Needs-of-Large-Language-Models" target="_blank" rel="external nofollow noopener">[논문 리뷰] DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <p>2024 acl-long</p> <h1 id="backgrounds">Backgrounds</h1> <ul> <li> <p><strong>Dynamic RAG</strong> 텍스트 생성 과정에서 <strong>검색 시점</strong>과, <strong>검색 내용</strong>을 능동적으로 결정</p> <p> 단일 검색에 의존하는 것이 아닌 보통 <strong>여러 차례 검색</strong>을 수행</p> <ul> <li>한계<ol> <li>검색 시점을 결정하는 전략은 종종 고정된 규칙에 의존 a. 불필요한 검색 증강이 LLM에 관련 없는 데이터나 노이즈를 추가하여 검색 품질 저하 b. LLM 추론 과정에서 연산 비용이 증강</li> <li>검색 내용을 결정하는 전략은 일반적으로 가장 최근 문장이나 마지막 몇 개의 토큰에만 한정됨 (LLM의 정보 요구는 전체 문맥에 걸쳐 있을 수 있음)</li> </ol> </li> <li>관련 연구<ul> <li> <strong>IRCoT</strong>: 생성되는 각 문장마다 검색을 수행하는 전역 증강 방식. 최신 생성 문장을 검색 질의로 설정함</li> <li> <strong>RETRO, IC-RALM</strong>: 슬라이딩 윈도우를 정의하고, 일정 개수의 토큰이 처리되었을 때 검색 모듈을 활성화하여, 마지막 N개의 토큰을 질의로 사용함</li> </ul> </li> </ul> </li> </ul> <hr> <h1 id="dragin">DRAGIN</h1> <p><strong>LLM의 정보 요구에 따라</strong> 검색 시점과 검색 내용을 결정</p> <p>별도의 추가 학습이나, 파인튜닝, 프롬프트 엔지니어링 없이도 모든 트랜스포머 기반 LLM에 쉽게 적용할 수 있는 경량의 RAG 프레임워크</p> <ol> <li>검색 시점 결정<ul> <li> <strong>RIND</strong>(<strong>R</strong>eal-time <strong>I</strong>nformation <strong>N</strong>eeds <strong>D</strong>etection)<ul> <li>LLM이 스스로 생성한 내용에 대한 불확실성, 각 토큰이 후속 토큰에 미치는 영향력, 그리고 각 토큰의 의미적 중요성을 종합적으로 고려</li> </ul> </li> </ul> </li> <li>검색 쿼리 구성(=검색 내용 결정)<ul> <li> <strong>QFS</strong>(<strong>Q</strong>uery <strong>F</strong>ormulation based on <strong>S</strong>elf-Attention)<ul> <li>전체 문맥에 걸친 LLM의 self-attention을 활용하여 쿼리 구성을 혁신적으로 개선</li> </ul> </li> </ul> </li> </ol> <hr> <h1 id="methodology">Methodology</h1> <h2 id="rind">RIND</h2> <p>각 토큰의 불확실성, 의미적 기여도, 이후 문맥에 미치는 영향을 종합적으로 평가하여 <strong>검색 활성화 여부를 결정</strong></p> <p>$S_{\text{RIND}}(t_i) = H_i \cdot a_{\text{max}}(i) \cdot s_i &gt; \theta$</p> <ol> <li> <p>각 토큰의 불확실성 정량화</p> <ul> <li> <p>$T = {t_1, t_2, …, t_n}$ </p> <ul> <li>출력 시퀀스 집합</li> </ul> </li> <li> <p>$H_i = - \sum_{v \in V} p_i(v) \log p_i(v)$</p> <ul> <li>특정 토큰 $t_i$에 대한 엔트로피로서, $p_i(v)$는 위치 $i$에서 LLM이 단어 집합 V 내의 토큰 $v$를 생성할 확률을 의미함 </li> </ul> </li> </ul> </li> <li> <p>이후 문맥에 미치는 영향력 정량화</p> <ul> <li> <p>Attention Matrix 계산</p> <ul> <li>$A = \text{softmax} \left( \text{mask} \left( \frac{QK^\top}{\sqrt{d_k}} \right) \right)$<ul> <li>$Q$: 쿼리 행렬</li> <li>$K$: key 행렬</li> <li>$d_k$: key 벡터의 차원 수</li> </ul> </li> </ul> </li> <li> <p>Maximum Attention Value 계산</p> <ul> <li>$a_{\text{max}}(i) = \max_{j&gt;i} A_{j,i}$ → 특정 토큰 $t_i$가 이후 문맥에 미치는 상대적인 영향력을 측정</li> </ul> </li> </ul> </li> </ol> <ol start="3"> <li> <p>각 토큰의 의미적 기여도 정량화</p> <ul> <li> <p>불용어를 필터링하여 의미적으로 중요한 토큰에 집중</p> <ul> <li> <p>$s_i = \begin{cases} 0, &amp; \text{if } t_i \in S \ 1, &amp; \text{otherwise} \end{cases}$</p> </li> <li> <p>$S$: 불용어 집합</p> </li> </ul> </li> </ul> </li> </ol> <h2 id="qfs">QFS</h2> <p>RIND에 의해 검색 위치가 결정되면, 해당 위치에서 검색을 위한 쿼리 재구성 Self-Attention 정보를 활용하여 모델이 가장 중요하게 여긴 부분을 자동으로 추출하여 검색 쿼리를 구성함</p> <ol> <li> <p><strong>마지막 트랜스포머 레이어에서 Attention Score 추출</strong></p> <ul> <li>특정 토큰 $t_i$를 생성할 때, 이전 토큰 중 어떤 것이 중요했는지를 Attention Weight를 분석함으로써 파악 가능 <ul> <li>$A_i = {a_{i,1}, a_{i,2}, \dots, a_{i,i-1}}$</li> </ul> </li> </ul> </li> <li> <p><strong>Attention Score 정렬</strong></p> <ul> <li>$A_i$를 내림차순으로 정렬하고, 가장 높은 점수를 받은 토큰 N개 선택<ul> <li><strong>$t_i$를 생성하는 데 가장 중요한 역할을 한 토큰들을 선별 및 추출</strong></li> </ul> </li> </ul> </li> <li> <p><strong>최종 검색 쿼리 구성</strong></p> <ul> <li>선택된 단어들을 원래 텍스트 순서대로 배치하여 검색 쿼리 $Q_i$ 생성</li> </ul> </li> </ol> <p>⇒ 단순히 마지막 문장이나 몇 개의 토큰을 쓰는 방식보다 <strong>전체 문맥을 고려</strong>하는 방식이 LLM이 실제로 필요로 하는 정보를 더 정확하게 검색할 수 있음</p> <h2 id="continue-generation-after-retrieval">Continue Generation after Retrieval</h2> <ol> <li>LLM이 생성하는 도중 외부 정보가 필요하면 RIND 모듈이 감지하여 검색을 수행</li> <li>QFS 모듈이 검색 쿼리를 생성하고, 검색 모델이 외부 데이터베이스에서 관련 문서를 가져옴</li> <li>LLM의 기존 생성 결과를 검색이 감지된 위치에서 잘라내고(truncate), 검색된 정보를 반영하여 생성 다시 시작</li> <li>이후에도 필요할 때마다 반복적으로 검색 및 생성이 진행</li> </ol> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/647fca86-1973-41a1-91a5-9805c2f0a9f9/image.png" alt="prompt"></p> <hr> <h1 id="experimental-setup">Experimental Setup</h1> <h2 id="datasets">Datasets</h2> <ul> <li> <strong>멀티홉 추론</strong><ul> <li>2WikiMultihopQA<ul> <li>메트릭: EM, F1, Precision</li> </ul> </li> <li>HotpotQA<ul> <li>메트릭: EM, F1, Precision</li> </ul> </li> </ul> </li> <li> <strong>읽기 이해</strong><ul> <li>IIRC<ul> <li>메트릭: EM, F1, Precision</li> </ul> </li> </ul> </li> <li> <strong>일반 상식 추론</strong><ul> <li>StrategyQA<ul> <li>메트릭: EM</li> </ul> </li> </ul> </li> </ul> <h2 id="baselines">Baselines</h2> <ul> <li>wo-RAG(without RAG)</li> <li>SG-RAG(Single Round RAG)</li> <li>FL-RAG(Fix Length RAG)<ul> <li>N개의 토큰이 생성될 때마다 검색 수행</li> </ul> </li> <li>FS-RAG(Fix Sentence RAG)<ul> <li>고정된 문장 단위로 검색 수행</li> </ul> </li> <li>FLARE<ul> <li>불확실한 토큰을 감지할 때마다 검색 수행<ul> <li>검색 모듈이 활성화되면, 생성된 문장에서 불확실한 토큰을 제외한 마지막 문장을 쿼리로 정의</li> </ul> </li> </ul> </li> </ul> <h2 id="selected-llms">Selected LLMs</h2> <ul> <li>LLaMA2-chat-7B, 13B</li> <li>Vicuna-13B-v1.5</li> </ul> <h2 id="implementation-details">Implementation Details</h2> <ul> <li>하이퍼파라미터<ul> <li>Appendix E 참조</li> </ul> </li> <li>Retriever<ul> <li>BM25<ul> <li>일부 밀집 검색 모델보다 더 우수한 성능을 보였다는 연구 결과</li> </ul> </li> <li>SGPT(Muennighoff, 2022)<ul> <li>SOTA 밀집 검색 모델</li> </ul> </li> </ul> </li> <li>불용어 처리<ul> <li>Spacy 라이브러리 en_core_web_sm 언어 모델 활용</li> </ul> </li> <li>외부 지식 코퍼스<ul> <li>Wikipedia<ul> <li>100개 토큰 단위로 분할하여 검색에 활용</li> </ul> </li> </ul> </li> </ul> <hr> <h1 id="results">Results</h1> <p><img src="https://velog.velcdn.com/images/yeomsee97/post/5d8816bf-40f8-4d0a-ac36-1f261982d535/image.png" alt="table 2"></p> <ol> <li> <p>wo-RAG &lt; SR-RAG</p> </li> <li> <p>FL-RAG, FS-RAG 방식이 항상 SR-RAG 방식보다 뛰어나지는 않음 즉, <strong>잘못된 시점에 검색 증강이 이루어질 경우, LLM의 출력 품질 향상에 기여하지 못할 수도 있음</strong>을 보여줌</p> </li> <li> <p>FLARE는 데이터셋에 따라 성능 편차가 큼 FLARE의 검색 증강 시점 결정 방식이 최적과는 거리가 멀다는 것을 의미함</p> </li> <li> <p>DRAGIN이 대부분 LLM 및 데이터셋에서 뛰어난 성능을 보임 심지어, <strong>멀티홉 QA에서 더욱 더 큰 향상</strong></p> </li> </ol> <hr> <p class="post-meta"> 1 min read   ·   April 06, 2025   ·   velog </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/velog"> <i class="fa-solid fa-hashtag fa-sm"></i> velog</a>   <a href="/blog/tag/technical"> <i class="fa-solid fa-hashtag fa-sm"></i> technical</a>   ·   <a href="/blog/category/external-posts"> <i class="fa-solid fa-tag fa-sm"></i> external-posts</a> </p> </li> </ul> <nav aria-label="Blog page navigation"> <ul class="pagination pagination-lg justify-content-center"> <li class="page-item disabled"> <a class="page-link" href="" tabindex="-1" aria-disabled="">&lt;</a> </li> <li class="page-item active"> <a class="page-link" href="/blog/" title="blog">1</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/2/" title="blog - page 2">2</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/3/" title="blog - page 3">3</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/4/" title="blog - page 4">4</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/5/" title="blog - page 5">5</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/2/">&gt;</a> </li> </ul> </nav> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Sihyeong Yeom. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> </body> </html>